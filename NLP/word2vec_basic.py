

# [NLP] 秒懂词向量Word2vec的本质
# https://zhuanlan.zhihu.com/p/26306795


# 【重点推荐】Xin Rong 的论文：『word2vec Parameter Learning Explained』

# 把词语转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫『词嵌入（word embedding)』，而 Word2vec，就是词嵌入（ word embedding) 的一种

# 大部分的有监督机器学习模型，都可以归结为：f(x) -> y
# 这里的 f，便是 NLP 中经常出现的『语言模型』（language model）
# 这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则。
# 更通俗点说就是：词语x和词语y放在一起，是不是人话。

# Word2vec 的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物—— 模型参数（这里特指神经网络的权重）
# 将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做—— 『词向量』

# 上述：『语言模型』f
# 如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『skip-gram 模型』
# 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』

# one-hot encoder：词汇表中词语的唯一表示

# Skip-gram 的网络结构：x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。

# 首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），训练这个神经网络，要用反向传播算法，本质上是链式求导

# 当模型训练完后，最后得到的其实是『神经网络的权重』，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，
# 则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，
# 从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。
# 就是 Word2vec 的精髓！！

# 这个词向量的维度（与隐含层节点数一致）一般情况下要远远小于词语总数 V 的大小，
# 所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。

# 可以看成是 单个x->单个y 模型的并联，cost function 是单个 cost function 的累加（取log之后）


# 为什么要用训练技巧trick（hierarchical softmax 和 negative sampling ）呢？
# Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，
# 但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。

# hierarchical softmax：本质是把 N 分类问题变成 log(N)次二分类
# negative sampling：本质是预测总体类别的一个子集


# 在词嵌入领域，除了 Word2vec之外，还有基于共现矩阵分解的 GloVe 等等词嵌入方法。
# 神经网络形式表示的模型（如 Word2vec），跟共现矩阵分解模型（如 GloVe），有理论上的相通性。所以在实际应用当中，这两者的差别并不算很大，
# 尤其在很多 high-level 的 NLP 任务（如句子表示、命名体识别、文档表示）当中，经常把词向量作为原始输入，而到了 high-level 层面，差别就更小了。

# 包括但不限于：
#
# 计算相似度
# 寻找相似词
# 信息检索
#
# 作为 SVM/LSTM 等模型的输入
# 中文分词
# 命名体识别
#
# 句子表示
# 情感分析
#
# 文档表示
# 文档主题判别

# 实战：
# 上面讲了这么多理论细节，其实在真正应用的时候，只需要调用 Gensim （一个 Python 第三方库）的接口就可以。

# Huffman tree 被证明是更高效、更节省内存的编码形式，所以相应的权重更新寻优也更快。
# 举个简单例子，高频词在Huffman tree中的节点深度比完全二叉树更浅，
# 比如在Huffman tree中深度为3，完全二叉树中深度为5，则更新权重时，Huffmantree只需更新3个w，而完全二叉树要更新5个，当高频词频率很高时，算法效率高下立判。



# https://www.zhihu.com/question/53011711
# word2vec 相比之前的 Word Embedding 方法好在什么地方？

# 举一个生活中的例子，语言模型和word2vec的关系可以类比于单反相机和美颜手机，它们的受众不一样。就照片质量（MLE）而言，单反肯定好。但如果更关心美颜（词嵌入）和便携性（训练速度），美颜手机就更受欢迎。