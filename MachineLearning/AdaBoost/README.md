《统计学习方法》李航

提升（boosting）方法，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。（弱 提升为 强）

思路："三个臭皮匠顶个诸葛亮"。"弱可学习"，学习的成功率比随机猜测略好。将"弱学习算法"提升为"强学习算法"。

最具代表性：AdaBoost算法

boosting方法的两个问题：
1、每一轮如何改变训练数据的权值或概率分布；
2、如何将弱分类器组合成一个强分类器。

AdaBoost的做法：
1、提高被前一轮弱分类器错分的样本的权值，降低正确分类的样本权值；
2、加权多数表决：加大分类错误率小的弱分类器权值

算法：
1、原始数据均匀分布，学习基本分类器G1(x)
2、对每一轮m = 1, 2,...,M：使用当前分布加权的数据，学习Gm(x)；计算分类误差率em；计算Gm(x)的系数αm（其中αm随em的减小而增大）；更新数据权值
3、线性组合f(x)实现M个分类器的加权表决。

AdaBoost基本的性质是能在学习过程中不断减少训练误差（指数速率下降）。它具有适应性，能使用弱分类器各自的训练误差率。
另一个解释：模型为学习加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。

前向分步算法：将同时求解从m=1到M的所有参数βm、rm的优化问题，简化为，逐次求解各个βm、rm的优化问题。
AdaBoost算法是前向分步算法的特例。


以决策树（DecisionTree）为基函数的提升方法称为 提升树（boosting tree）。
提升树模型：可表示为决策树的加法模型。
回归问题的提升树算法：数据切分点；残差rx；平方损失误差；

对于一般损失函数（非平方损失和指数损失），每一步优化不容易，提出：梯度提升（gradient boosting）算法。
利用最速下降法的近似方法，关键：损失函数的负梯度作为残差的近似值。







《机器学习实战 Machine Learning in Action》

AdaBoost：最流行的元算法。
元算法（meta-algorithm）：吸取多个专家意见，对其它算法进行组合的一种方式。
集成方法（ensemble method）或元算法包括：
1、bagging方法（自举汇聚法 bootstrap aggregating）：S个数据集，S个分类器，投票结果最多的类别作为最后的分类结果。
2、boosting：集中关注被已有分类器错分的那些数据，来获得新的分类器。结果是基于所有分类器的加权求和。最流行的版本AdaBoost.

训练算法：
AdaBoost（Adaptive Boosting，自适应），不断重复训练和调整权重，直到错误率为0或弱分类器数目达到用户指定值为止。


单层决策树（decision stump，也称 决策树桩）：仅基于单个特征来做决策。只有一次分裂过程。

AdaBoost 可基于单层决策树构建弱分类器，组合多个单层决策树，正确分类。

buildStump()

adaBoostTrainDS()   # DS(Decision stump)，它是AdaBoost中最流行的弱分类器。实际上，任意分类器均可作为基分类器。


测试算法：
已经拥有了多个弱分类器及其对应alpha值。
对每个弱分类器，输出类别估计值 * alpha值，累加。

使用50个弱分类器，达到了很好的效果。远超Logistic回归。
但数目太大，容易过拟合。




# 非均衡分类问题：：：：：：
针对所有分类问题。
大多数情况下，不同类别的分类代价，并不相等。

新的分类器性能度量方法：
之前都是基于错误率。混淆矩阵（confusion matrix）工具，帮助人们更好地了解分类中的错误。
当某个类别的重要性更高时，可以定义出多个比错误率更好的新指标：
1、正确率（Precison）：预测为正例的样本中的真正正确的 比例
2、召回率（Recall）：预测为正例的真实正例 占 所有真实正例的 比例。

故召回率很大的分类器中，判错的正例很少。将任何样本均判为正例，则召回率百分之百；但正确率很低。
让1、2都高，很难同时达到。

另一个度量指标：
3、ROC曲线（ROC curve）：代表接收者操作特征（receiver operating characteristic）
x-y：假阳率-真阳率
两条线：实线-虚线。虚线为随机猜测的结果曲线。

最佳分类器，尽可能位于左上角。

对不同ROC曲线，比较的指标是：AUC（Area Unser the Curve, 曲线下的面积）：给出分类器的平均性能值。完美为1.0；随机为0.5

plotROC()  # 绘制ROC曲线，及计算AUC



基于代价函数，分类器决策控制：
除了调节分类器的阈值之外，还可以：代价敏感学习，选择付出最小代价的分类器。
算法中引入代价信息，给较小类更多权重。即在训练时，小类当中只允许更少的错误。


处理非均衡问题的数据抽样方法：
改造训练数据，欠抽样（删除样例）、过抽样（复制样例）


