《统计学习方法》李航

隐马尔科夫模型（hidden Markov model, HMM）：
生成模型，隐藏的马尔科夫链 随机生成观测序列的过程。


具体过程：隐藏的马尔科夫链 随机生成 不可观测的状态序列（state sequence）；每个状态生成一个观测；从而产生观测序列（observation sequence）。
观测序列的每一个位置，可以看作一个时刻。（关于时序的概率模型）

模型：初始概率分布π；状态转移概率分布A；观测概率分布B
模型γ = （A, B, π） 三要素

π和A 决定状态序列；B决定观测序列。

两个基本假设：
1、齐次马尔科夫假设：时刻t的状态，只依赖于前一时刻的状态；
2、观测独立性假设：任何时刻的观测，只依赖于该时刻的马尔科夫链的状态。

用于标注问题的学习模型：标准问题是给定观测序列，预测其对应的标记序列。（HMM中 状态对应标记）。

（盒子和球模型）：
两个随机序列：
1、盒子的序列（状态序列、隐藏的）；
2、球的颜色的观测序列（可观测的）。


观测序列的生成过程：输入隐马尔科夫模型γ = （A, B, π）、观测序列长度；输出O={o1, o2, ..., oT}
1、按照初始分布π 产生状态i1；
2、令T=1；
3、按照iT的观测概率分布b 生成oT；
4、按照iT的状态转移概率分布a 产生状态i T+1:;
5、令T = T + 1 转至（3）



隐马尔科夫模型的三个基本问题：
1、概率计算问题：P(O | γ)，即 P(O | (A, B, π))；
2、学习问题：已知O，估计模型γ = （A, B, π）的参数，使得P（O | γ）最大；
3、预测问题：也成为解码（decoding）问题。给定观测序列，求最有可能的对应的状态序列。P(I | O)


概率计算问题 P(O | γ)：前向-后向算法（forward-backward）

学习问题（参数估计）：
1、监督学习：训练数据包含观测序列、对应的状态序列
2、非监督学习：训练数据只包含观测序列；Baum-Welch算法（也即是「EM算法」）；引入隐数据I

预测问题（P(I | O)）：
1、近似算法：每个时刻T，选择最有可能出现的状态iT
2、维特比Viterbi算法：动态规划求概率最大路径（最优路径），此时一条路径对应着一个状态序列。

最佳实践：
from pomegranate import State, HiddenMarkovModel