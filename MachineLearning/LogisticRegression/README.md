《统计学习方法》李航
逻辑斯谛回归（Logsitic regression）是经典分类方法。

逻辑斯谛分布（Logsitic distribution）：
X服从逻辑斯谛分布是指X具有以下分布函数F(x)和密度函数f(x)：
分布函数属于逻辑斯蒂函数，S形曲线（sigmoid curve）。

二分逻辑回归的参数估计法，也可以推广到多项逻辑回归。

模型学习的最优化算法：改进的迭代尺度法IIS、梯度下降法、牛顿法或拟牛顿法。






《机器学习实战 Machine Learning in Action》
最优化算法。

假设有一些数据点，用一条直线对它们进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。

Sigmoid函数：
y = 1 / (1 + e^-z)
y = 0.5，当x = 0时。
y属于0~1；x增大时，y快速逼近1；x减小时，y快速逼近于0. x = 0处，simoid函数看起来很像阶跃函数（0-1）

为了实现Logistic回归分类器，可以在每个特征x上，都乘以回归系数，然后把所有结果相加，总和代入sigmoid函数。大于0.5的数据被分为1类，小于0.5的归入0类。所以是一种概率估计。

输入 z = w0 x0 + w1 x1 + ... + wn xn
向量W为最佳参数（系数）

最优化算法：（W的确定）
梯度上升法：迭代更新 w := w + α * 梯度
梯度下降法：w := w - α * 梯度

计算真实类别与预测类别的差值，按照该差值的方向调整回归系数

随机梯度上升算法：一次仅用1个样本点来更新参数。在线学习算法。有局部的波动现象。

改进的随机梯度上升算法：
alpha = 4/(1.0 + i + j) + 0.01  //每次迭代都调整，不断减小，但不会为0，会缓解数据波动或者高频波动。
随机选取样本来更新回归系数。减少周期性的波动。

案例中，有30%的值缺失。
处理方案：
1、所有缺失值必须用一个实数0替换；
2、测试数据集中的类别标签缺失的数据，直接丢弃。


逻辑回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数。求解过程由最优化算法来完成。

