《统计学习方法》李航
k近邻法（k-nearest neighbor, k-NN），基本分类和回归方法。
假设给定的训练数据集中，实例类别已定。分类时，对于新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。
没有显式的学习过程。
3个基本要素：k值的选择，距离度量，分类决策规则。
1968年提出。

k = 1，最近邻算法。k越小，整体模型越复杂，容易过拟合；通常采用交叉验证法来选取k
Lp距离：p = 2，欧氏距离；p = 1，曼哈顿距离；p = ∞，各个坐标距离的最大值
分类决策规则：一般是多数表决（majority voting rule）

实现：线性扫描（最简单、耗时）、kd树（二叉树）



《机器学习实战 Machine Learning in Action》
k-近邻算法采用测量不同特征值之间的距离进行分类。

分析数据：使用Matplotlib创建散点图 scatter

准备数据：数值归一化，对于不同取值范围的特征值

案例：MNIST手写数字识别

缺点：耗时；无法给出任何数据的基础结构信息






