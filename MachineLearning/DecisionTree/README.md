《统计学习方法》李航
可以认为是if-then规则的集合，也可以认为是定义在特征与类空间上的条件概率分布。
主要优点是模型具有可读性，分类速度快。

3个步骤：特征选择，决策树的生成，树的修剪

决策树由结点node、有向边directed edgy组成。内部结点-特征；叶结点-类别

选取最优决策树是NP完全问题，近似求解。

树的生成，递归选择最优特征，据此分隔训练数据。直到所有数据子集正确分类、或者没有合适的特征为止。

特征选择准则：信息增益、信息增益比。

熵entropy：随机变量不确定性的度量。熵越大，不确定性越大。
条件熵 H（Y|X）：已知随机变量X的条件下，随机变量Y的不确定性。
信息增益（information gain）表示得知特征X的信息，而使得类Y的信息不确定性减少的程度。由于特征A，而使得数据集的分类的不确定性减少的程度。
信息增益大的特征，具有与更强的分类能力。

信息增益准则，存在偏向于选择取值较多的特征的问题？？故使用信息增益比，来校正。

特征A对训练数据集合D的信息增益（信息增益比）。


决策树的生成：
ID3算法：选择信息增益最大的特征作为结点的特征。相当于用极大似然法进行概率模型的选择。
直到所有特征的信息增益均很小（低于阈值）、或没有特征可以选择为止。

C4.5算法：选择信息增益比

决策树的剪枝pruning：
防止过拟合。
往往通过极小化决策树整体的损失函数来实现。依赖：经验熵
损失函数 = 预测误差 + 模型复杂度。
自底向上，从叶结点向上回缩，剪掉不必要的叶结点。

CART算法（分类与回归树 classification and regression tree）：1984年提出，基尼指数Gini()，待后续跟进...




《机器学习实战 Machine Learning in Action》

最经常使用的数据挖掘算法。
主要优势是数据形式非常容易理解。
根据数据集创建规则的过程，就是机器学习的过程。

构造决策树：
第一个问题是，哪个特征在划分数据分类时起决定性作用。
如何划分，何时停止划分？

划分大原则：将无序的数据变得更加有序。

信息论之父 克劳德-香农。
熵（香浓熵）：信息的期望值。l(xi) = -log2p(xi)  其中，p(xi)是选择分类i的概率 = 该分类样本数 / 总数

另一个度量集合无序程度的方法是 基尼不纯度 Gini impurity。CART？

信息增益：熵的减少、数据无序度的减少。

递归的终止条件：所有的类标签完全相同；使用完了所有特征（此时挑选出现次数最多的类别返回）。

Matplotlib提供了一个非常有用的注解工具annotations，可以在数据图形上添加文本注释。

按照叶子节点数将x轴划分为若干部分。按照图形比例绘制树形图（x为0.0-1.0，y为0.0-1.0）。

存储分类器：python的pickle序列化对象。pickle.dump(inputTree, file)










